# Prometheus monitoring stack configuration
# Deployed via kube-prometheus-stack Helm chart
#
# helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
# helm install monitoring prometheus-community/kube-prometheus-stack \
#   -n monitoring --create-namespace \
#   -f kubernetes/monitoring/prometheus-values.yaml

prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    retention: 15d
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 1000m
        memory: 4Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

grafana:
  enabled: true
  adminPassword: admin
  persistence:
    enabled: true
    size: 10Gi
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: data-platform
          orgId: 1
          folder: "Data Platform"
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/data-platform

alertmanager:
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'slack-notifications'
      routes:
        - match:
            severity: critical
          receiver: 'slack-critical'
          repeat_interval: 1h
    receivers:
      - name: 'slack-notifications'
        slack_configs:
          - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
            channel: '#data-platform-alerts'
            title: '{{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
      - name: 'slack-critical'
        slack_configs:
          - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
            channel: '#data-platform-critical'
---
# ServiceMonitor for Event Collector
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: event-collector
  namespace: data-platform
  labels:
    app: event-collector
spec:
  selector:
    matchLabels:
      app: event-collector
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
---
# PrometheusRule for data platform alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: data-platform-alerts
  namespace: data-platform
spec:
  groups:
    - name: data-pipeline-alerts
      rules:
        # High error rate in event collection
        - alert: EventCollectorHighErrorRate
          expr: |
            rate(events_published_total{status="error"}[5m])
            / rate(events_published_total[5m]) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Event collector error rate > 5%"
            description: |
              Event collector {{ $labels.instance }} has error rate of
              {{ $value | humanizePercentage }} over the last 5 minutes.

        # Pipeline execution failures
        - alert: PipelineExecutionFailed
          expr: pipeline_runs_total{status="error"} > 0
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Pipeline {{ $labels.pipeline_name }} failed"

        # Data freshness SLA breach
        - alert: DataFreshnessSLABreach
          expr: data_freshness_minutes > 30
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Data freshness SLA breach for {{ $labels.table_name }}"
            description: |
              Table {{ $labels.table_name }} data is {{ $value }} minutes old.
              SLA threshold: 30 minutes.

        # Pod restart alert
        - alert: DataPlatformPodRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{
              namespace="data-platform"
            }[1h]) > 3
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.pod }} restarting frequently"
